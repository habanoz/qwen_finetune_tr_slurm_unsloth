{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e6b1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasketch in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (1.6.5)\n",
      "Requirement already satisfied: datasets in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasketch) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasketch) (1.13.1)\n",
      "Requirement already satisfied: filelock in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (3.14.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: packaging in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (1.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/melek/miniconda3/envs/myenv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasketch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960bceb",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14460ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "768e9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = load_dataset(\"habanoz/WildChat-turkce\")['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94f2c8f",
   "metadata": {},
   "source": [
    "## Normalize Chat Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ed87fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversation_hash', 'model', 'timestamp', 'conversation', 'turn', 'language', 'openai_moderation', 'detoxify_moderation', 'toxic', 'redacted', 'state', 'country', 'hashed_ip', 'header'],\n",
       "    num_rows: 6104\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2510ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_chat(chat):\n",
    "    return \"\\n\".join(f\"{t['role']}: {t['content']}\" for t in chat)\n",
    "\n",
    "ds = ds.map(lambda x: {'chat': flatten_chat(x['conversation'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b4a268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversation_hash', 'model', 'timestamp', 'conversation', 'turn', 'language', 'openai_moderation', 'detoxify_moderation', 'toxic', 'redacted', 'state', 'country', 'hashed_ip', 'header', 'chat'],\n",
       "    num_rows: 6104\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81578e29",
   "metadata": {},
   "source": [
    "## Remove Exact Deduplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8249709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 6104\n",
      "Unique: 6092\n",
      "Exact duplicates: 12\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = ds.to_pandas()\n",
    "print(\"Original:\", len(df))\n",
    "deduped = df.drop_duplicates(\"chat\")\n",
    "print(\"Unique:\", len(deduped))\n",
    "print(\"Exact duplicates:\", len(df) - len(deduped))\n",
    "\n",
    "ds_dedup = Dataset.from_pandas(deduped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f934d7c",
   "metadata": {},
   "source": [
    "## Near Deduplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7393c1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Near-duplicate groups (whole chats): 110\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "def get_minhash(text, num_perm=128):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    for word in text.split():\n",
    "        m.update(word.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "lsh = MinHashLSH(threshold=0.9, num_perm=128)\n",
    "minhashes = {}\n",
    "\n",
    "for i, t in enumerate(ds_dedup[\"chat\"]):\n",
    "    m = get_minhash(t)\n",
    "    lsh.insert(i, m)\n",
    "    minhashes[i] = m\n",
    "\n",
    "dup_groups = []\n",
    "seen = set()\n",
    "for i in minhashes:\n",
    "    if i not in seen:\n",
    "        near_dups = lsh.query(minhashes[i])\n",
    "        if len(near_dups) > 1:\n",
    "            dup_groups.append(near_dups)\n",
    "        seen.update(near_dups)\n",
    "\n",
    "print(\"Near-duplicate groups (whole chats):\", len(dup_groups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7daf486c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chats to remove: 244\n"
     ]
    }
   ],
   "source": [
    "to_remove = set()\n",
    "for group in dup_groups:\n",
    "    # keep the first one, remove the others\n",
    "    for idx in group[1:]:\n",
    "        to_remove.add(idx)\n",
    "\n",
    "print(\"Chats to remove:\", len(to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c90a46ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: 6092\n",
      "After deduplication: 5848\n"
     ]
    }
   ],
   "source": [
    "keep_idx = [i for i in range(len(ds_dedup)) if i not in to_remove]\n",
    "ds_near_dedup = ds_dedup.select(keep_idx)\n",
    "\n",
    "print(\"Original:\", len(ds_dedup))\n",
    "print(\"After deduplication:\", len(ds_near_dedup))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1730bc76",
   "metadata": {},
   "source": [
    "## Duplicate turns (inside conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc2df27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique turns: 14997\n",
      "Repeated turns: 104\n",
      "Most common turns: [('Merhaba! Size nasıl yardımcı olabilirim?', 233), ('Hello! How can I assist you today?', 39), ('Merhaba, nasıl yardımcı olabilirim?', 8), ('Merhaba, size nasıl yardımcı olabilirim?', 8), ('Merhaba! Nasıl yardımcı olabilirim?', 7), ('Evet, Türkçe biliyorum. Size nasıl yardımcı olabilirim?', 7), ('Evet, Türkçe konuşabilirim. Size nasıl yardımcı olabilirim?', 6), ('Doğru.', 6), ('Doğru', 5), ('Hello! How can I help you today?', 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_turns = []\n",
    "for conv in ds_near_dedup[\"conversation\"]:\n",
    "    for t in conv:\n",
    "        if t[\"role\"] == \"assistant\":  # or include user too\n",
    "            all_turns.append(t[\"content\"].strip())\n",
    "\n",
    "turn_counts = Counter(all_turns)\n",
    "repeated_turns = [(t, c) for t, c in turn_counts.items() if c > 1]\n",
    "\n",
    "print(\"Unique turns:\", len(turn_counts))\n",
    "print(\"Repeated turns:\", len(repeated_turns))\n",
    "print(\"Most common turns:\", turn_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad1c9de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting_turns = set(turn  for turn, count in turn_counts.most_common(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c44fe8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Evet, Türkçe biliyorum. Size nasıl yardımcı olabilirim?',\n",
       " 'Evet, Türkçe konuşabilirim. Size nasıl yardımcı olabilirim?',\n",
       " 'Hello! How can I assist you today?',\n",
       " 'Merhaba! Nasıl yardımcı olabilirim?',\n",
       " 'Merhaba! Size nasıl yardımcı olabilirim?',\n",
       " 'Merhaba, nasıl yardımcı olabilirim?',\n",
       " 'Merhaba, size nasıl yardımcı olabilirim?'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greeting_turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21ad026e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dbbc1294e84ff98dcf74e89968b6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96734648c904a9bb50abfd432382c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "turns_removed = 0\n",
    "def remove_greeting_turns(example):\n",
    "    global turns_removed\n",
    "    if example['conversation'][1]['content'] in greeting_turns:\n",
    "        example['conversation'] = example['conversation'][2:]  # remove first and second turn that involves greeting\n",
    "        turns_removed+=2\n",
    "    return example\n",
    "\n",
    "ds_near_dedup = ds_near_dedup.map(remove_greeting_turns)\n",
    "ds_near_dedup = ds_near_dedup.filter(lambda x: len(x['conversation']) > 1)  # keep only if more than 1 turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b1bd9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5d626fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cfe54b11be4196929b9b9bd37b7eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5844 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "turns_removed = 0\n",
    "def remove_greeting_turns(example):\n",
    "    global turns_removed\n",
    "    if example['conversation'][1]['content'] in greeting_turns:\n",
    "        example['conversation'] = example['conversation'][2:]  # remove first and second turn that involves greeting\n",
    "        turns_removed+=2\n",
    "    return example\n",
    "\n",
    "ds_near_dedup = ds_near_dedup.map(remove_greeting_turns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6abd10",
   "metadata": {},
   "source": [
    "## Remove Political Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a09c304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_remove = list()\n",
    "for i, row in enumerate(ds_near_dedup):\n",
    "    if set(['tayyip','erdoğan']) & set(row['chat'].lower().split()):\n",
    "        indices_to_remove.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67624ed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1a53b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_idx = [i for i in range(len(ds_near_dedup)) if i not in indices_to_remove]\n",
    "ds_near_dedup = ds_near_dedup.select(keep_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f4e0cf",
   "metadata": {},
   "source": [
    "## Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3686b37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orignal dataset size: 6104\n",
      "After cleanup: 5838\n",
      "Percentage removed: 4.3577981651376145\n"
     ]
    }
   ],
   "source": [
    "print(\"Orignal dataset size:\", len(ds))\n",
    "print(\"After cleanup:\", len(ds_near_dedup))\n",
    "print(\"Percentage removed:\", (len(ds) - len(ds_near_dedup)) / len(ds) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea436d4f",
   "metadata": {},
   "source": [
    "## Anonymize PII Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10c6707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "patterns = {\n",
    "    \"EMAIL\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\",\n",
    "    \"PHONE\": r\"(?:\\+?\\d{1,3}[-.\\s]?)?(?:\\(?\\d{2,4}\\)?[-.\\s]?)?\\d{3,4}[-.\\s]?\\d{4}\",\n",
    "    \"CREDIT_CARD\": r\"\\b(?:\\d[ -]*?){13,19}\\b\",\n",
    "    \"TCK\": r\"\\b\\d{11}\\b\",\n",
    "    \"IP\": r\"\\b(?:\\d{1,3}\\.){3}\\d{1,3}\\b\",\n",
    "}\n",
    "\n",
    "anonymized = {\n",
    "    \"EMAIL\" : \"someone@gmail.com\",\n",
    "    \"PHONE\": \"+90 555 555 5555\",\n",
    "    \"CREDIT_CARD\": \"4111 1111 1111 1111\",\n",
    "    \"TCK\": \"12345678901\",\n",
    "    \"IP\": \"192.168.1.0\"\n",
    "}\n",
    "\n",
    "n_cleaned = 0\n",
    "\n",
    "def redact_pii(text):\n",
    "    global n_cleaned\n",
    "    for label, pattern in patterns.items():\n",
    "        clean_text = re.sub(pattern, f\"<{anonymized[label]}>\", text, flags=re.IGNORECASE)\n",
    "    n_cleaned += (text != clean_text)\n",
    "    return clean_text\n",
    "\n",
    "def redact_chat(conversation):\n",
    "    for con in conversation:\n",
    "        con['content'] = redact_pii(con['content'])\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "384d2a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46576aff59a44b82a5aeeb32e4a1d8dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_no_pii = ds_near_dedup.map(lambda x: {'conversation': redact_chat(x['conversation'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4408cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b7ca0",
   "metadata": {},
   "source": [
    "## Save Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27ee1157",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ds= ds_no_pii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3be1d476",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ds = final_ds.remove_columns(set(final_ds.features.keys())-set([\"conversation\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8695ecfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['conversation'],\n",
       "    num_rows: 5838\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "993422ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e55c2e3f814c6292825194b47cb3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_ds = final_ds.map(lambda x: {'conversation':  [ {'role':msg['role'], 'content': msg['content']} for msg in x['conversation']]})  # remove any turn data except role and content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "255812f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1ce753c3b84b75b033c6ff5738f9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d03af46b158a44a493e5799c0f0e8538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/6 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files as a binary IO buffer is not supported by Xet Storage. Falling back to HTTP upload.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/habanoz/WildChat-turkce-cleaned/commit/d40f5211ec134b08a098af499dfcd09e7725a8d5', commit_message='Upload dataset', commit_description='', oid='d40f5211ec134b08a098af499dfcd09e7725a8d5', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/habanoz/WildChat-turkce-cleaned', endpoint='https://huggingface.co', repo_type='dataset', repo_id='habanoz/WildChat-turkce-cleaned'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_ds.push_to_hub(\"habanoz/WildChat-turkce-cleaned\", private=False, token=\"xxxx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
